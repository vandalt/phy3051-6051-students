{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c18523-0ad7-4e35-b3e1-03b13b1ae22b",
   "metadata": {},
   "source": [
    "# Modélisation de bruit corrélé autour d'un modèle physique avec les processus gaussiens\n",
    "\n",
    "Tel que vu en classe, un processus gaussien permet de modéliser le bruit corrélé autour d'un modèle paramétrique (souvent appelé \"mean function\" dans la litérature).\n",
    "Nous allons effectuer une telle analyse avec un modèle assez similaire à celui utilisé dans le devoir 1.\n",
    "\n",
    "Pour couvrir du même coup les GPs et le HMC, nous allons utilisé la librarie `tinygp`, qui utilise `jax` afin de facilement calculer le gradient des modèles GPs.\n",
    "Pour l'échantillonage HMC, nous allons utiliser NumPyro, une librairie de programmation probabilistique (PPL).\n",
    "\n",
    "## Définition du modèle physique\n",
    "Supposons qu'on cherche l'amplitude, la largeur et la position d'une raie d'émission. On additionne aussi une constante pour modéliser le niveau du continuum. Notre modèle sera donc\n",
    "\n",
    "$$\n",
    "m(x) = b + a \\exp{\\left(-\\frac{(x-\\ell)^2}{2 w^2}\\right)}\n",
    "$$\n",
    "\n",
    "Pour que le modèle soit compatible avec `tinygp` et NumPyro, nous allons le créer avec `jax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a337ae-4a30-4517-9b6f-c7c12b6074d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Précision machine \"double\" avec Jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "def gaussian_model(params: dict, X: float):\n",
    "    \"\"\"\n",
    "    Modèle d'une gaussienne et d'un terme constant.\n",
    "    \n",
    "    Paramètres\n",
    "    ----------\n",
    "    params : dict\n",
    "        Dictionaire de paramètres avec des éléments \"loc\", \"log_width\", \"a\" et \"b\".\n",
    "    X : float\n",
    "        Valeur(s) X pour laquelle on calcule la fonction.\n",
    "    \"\"\"\n",
    "    # Le modèle de gaussienne\n",
    "    mod = params[\"b\"] + params[\"a\"] * jnp.exp(-0.5 * jnp.square((X - params[\"loc\"]) / jnp.exp(params[\"log_width\"])))\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ed58e-b9db-42d4-847c-9ca3caef1bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim_params = {\n",
    "    \"a\": 0.3,\n",
    "    \"b\": 0.1,\n",
    "    \"loc\": 5.0,\n",
    "    \"log_width\": np.log(0.5),\n",
    "}\n",
    "\n",
    "X_grid = np.linspace(0, 10, num=200)\n",
    "\n",
    "ymodel = gaussian_model(sim_params, X_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a935061-f7fb-47e5-84e9-e8e43194396f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(X_grid, ymodel)\n",
    "plt.title(\"Modèle gaussien paramétrique\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9780d7-f659-4c32-b7f4-60bd7cafe385",
   "metadata": {},
   "source": [
    "## Données simulées\n",
    "On peut générer des données simulées à partir de la gaussienne, mais aussi d'un signal additionnel \"inconnu\" qu'on pourra modéliser avec un GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528077f4-14ca-45ef-84cd-ee048c7ba5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random = np.random.default_rng(135)\n",
    "\n",
    "# Données aléatoires\n",
    "X = np.sort(random.uniform(0, 10, 50))\n",
    "y = gaussian_model(sim_params, X)\n",
    "y += 0.1 * np.sin(2 * np.pi * (X - 5) / 10.0)\n",
    "y += 0.03 * random.normal(size=len(X))\n",
    "yerr = np.abs(0.003 * random.normal(size=len(X)) + 0.01)\n",
    "\n",
    "# Vrai signal, pour affichage\n",
    "X_test = np.linspace(X.min() - 2.0, X.max() + 2.0, num=200)\n",
    "true_y = gaussian_model(sim_params, X_test)\n",
    "true_y += 0.1 * np.sin(2 * np.pi * (X_test - 5) / 10.0)\n",
    "\n",
    "plt.errorbar(X, y, yerr, fmt=\"k.\", capsize=2)\n",
    "plt.title(\"Données simulées\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4af2b1-c3ee-416c-8e48-2347d3b9ec4a",
   "metadata": {},
   "source": [
    "## Modèle GP\n",
    "\n",
    "Il est maintenant temps de définir le modèle GP que nous allons utiliser pour les données.\n",
    "La fonction \"moyenne\" reste la même (une gaussienne), mais on ajoute un GP à notre modèle pour s'occuper du signal additionnel.\n",
    "On utilise un kernel Matern 5/2. En partique, on en testerait probablement quelques uns pour vérifier que le choix est optimal.\n",
    "\n",
    "On regénère le GP dans une fonction afin de facilement mettre à jour les paramètres à chaque itération d'un optimiseur ou MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca00c4-ac9b-4975-8ca7-13fdb90a2630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tinygp import kernels, GaussianProcess\n",
    "\n",
    "def build_gp(params: dict, X: np.ndarray, yerr: Union[np.ndarray, float] = 0.0) -> GaussianProcess:\n",
    "    \"\"\"\n",
    "    Création d'un modèle GP\n",
    "    \n",
    "    Paramètres\n",
    "    ----------\n",
    "    params : dict\n",
    "        Paramètres du modèle dans un dictionaire. Devrait contenir les paramètres log_gp_amp, log_gp_scale et log_gp_diag, en plus des paramètres pour gaussian_model()\n",
    "    X : np.ndarray\n",
    "        Coordonées d'entrée X du GP pour le calcul de la vraisemblance\n",
    "    yerr: Union[np.ndarray, float]\n",
    "        Erreur sur les mesures. Le carré des erreurs est ajouté à la diagonale.\n",
    "        Pour générer des échantillons du GP. ce paramètre devrait être 0.0\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Définition du GP \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a601aa-5882-4960-a2d4-bd89ded07438",
   "metadata": {},
   "source": [
    "### Distribution_prior_ du GP\n",
    "On peut d'abord vérifier de quoi on l'air les échantillons individuels du GP _à priori_, avant d'avoir montré des données au modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5df0f-5b40-48fd-a382-14ae9e98fbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_ini = {\n",
    "    \"log_gp_amp\": np.log(0.1),\n",
    "    \"log_gp_scale\": np.log(3),\n",
    "    # Petite valeur pour afficher des prédictions qui ne tiennent pas compte de données.\n",
    "    # Si on utilisait une grande valeur, il y aurait un bruit \"blanc\" (gaussien indépendant) sur chacun des points dans l'échantillon prior.\n",
    "    \"log_gp_diag\": np.log(1e-8),\n",
    "    \"a\": 0.2,\n",
    "    \"b\": 0.1,\n",
    "    \"loc\": 4.5,\n",
    "    \"log_width\": np.log(1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ed2ee-17a2-4803-8f1b-3534afda2def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Créer un GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c41558-6073-48bf-bd67-6af78a7d09e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Échantillonner le prior et afficher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffe134-b223-4f5c-8ffd-6062cf092674",
   "metadata": {
    "tags": []
   },
   "source": [
    "On peut également vérifier de quoi ont l'air la moyenne et la variance du GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d676b4f-6bd5-46b3-a62d-b5421b39ea04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Calculer moyenne et écart type du GP\n",
    "\n",
    "ysamp = gp_sample.sample(jax.random.PRNGKey(13), shape=(5,))\n",
    "\n",
    "plt.errorbar(X, y, yerr, fmt=\"k.\", capsize=2)\n",
    "plt.plot(X_test, ysamp.T)\n",
    "# TODO: Afficher 1 sigma GP\n",
    "plt.title(\"Données simulées\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2287808-07bd-4327-8e14-7745c04609d8",
   "metadata": {},
   "source": [
    "On voit que le prior du GP est très incertain autour de la moyenne. En montrant des données au GP, la situation devrait s'améliorer un peu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725f6f6-6df9-4149-ad64-319520eace15",
   "metadata": {},
   "source": [
    "### GP conditioné (_posterior_)\n",
    "Maintenat, _sans modifier les hyperparamètres_, on peut conditionner le GP sur les données. Mêmes si les hyperaparamètres ne sont pas optimaux, on devrait au moins obtenir une prédiction qui ne passe pas trop loin des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c3f37-e86c-4112-a1f2-c95dcb651328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ajout d'un terme de diagonale, qu'on avait initialisé à 0 pour tirer des échantillons\n",
    "params_ini[\"log_gp_diag\"] = np.log(0.001)\n",
    "gp = build_gp(params_ini, X, yerr=yerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf9bec-338c-414f-9aa4-8bff8b7a8b38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Conditionner le GP sur y et prédire à Xtest\n",
    "print(cond, cond_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9bc8a-f136-4971-9782-947646140207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csamp = cond_gp.sample(jax.random.PRNGKey(23), shape=(5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2ee20-1330-4734-b785-b878f4191035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "std = np.sqrt(cond_gp.variance)\n",
    "\n",
    "plt.errorbar(X, y, yerr, fmt=\"k.\", capsize=2)\n",
    "plt.plot(X_test, cond_gp.mean)\n",
    "plt.plot(X_test, true_y, \"k\", lw=1.5, alpha=0.3, label=\"Signal simulé\")\n",
    "plt.fill_between(X_test, cond_gp.mean - std, cond_gp.mean + std, alpha=0.5)\n",
    "plt.plot(X_test, csamp.T)\n",
    "plt.title(\"GP conditionné\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68751534-a9d7-4ee0-9217-6bbc8efd3300",
   "metadata": {},
   "source": [
    "Déjà beaucoup mieux. Le modèle passe maintenant plus près des données. L'incertitude est également contrainte.\n",
    "Par contre, on peut tout de même noter un petite écart dans le pic de la gaussienne. Et les échantillons oscillent un peu rapidement, faisant perdre de la capacité d'extrapolation.\n",
    "De meilleurs hyperparamètres devraient améliorer la situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21384a9f-a3b4-4f78-93e4-5bcc2b12602a",
   "metadata": {},
   "source": [
    "## Optimisation avec Jaxopt\n",
    "\n",
    "Il existe plusieurs librairies d'optimisation avec Jax. Ici, on utilise Jaxopt, comme dans la documentation de `tinygp` pour ce tutoriel.\n",
    "Comme Jax calcule automatiquement les gradients, on peut tirer avantage des algorithmes d'optimisation avec gradient.\n",
    "\n",
    "On doit d'abord définir une \"loss function\", que l'on souhaite minimiser. Il s'agit ici de la vraisemblance négative (toujours en log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfa448-098b-44bc-9d40-ce59fc39de19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# jax.jit signifie qu'on souhaite utiliser la compilation \"Just in time\"\n",
    "@jax.jit\n",
    "def loss(params, X, yerr):\n",
    "    gp = build_gp(params, X, yerr)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "loss(params_ini, X, yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2cd357-91b0-42ba-898b-965844ce61c8",
   "metadata": {},
   "source": [
    "On peut ensuite chercher le minimum avec une interface similaire à scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5b5e9-9568-490f-853b-92641fd2241c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jaxopt\n",
    "\n",
    "solver = jaxopt.ScipyMinimize(fun=loss)\n",
    "soln = solver.run(jax.tree_util.tree_map(jnp.asarray, params_ini), X, yerr)\n",
    "print(f\"Log-likelihood négatif final: {soln.state.fun_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688a0c1-079d-4aff-be08-3eb1a5e1182a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soln.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b3097-5bf2-4587-addc-bd518c010a7f",
   "metadata": {},
   "source": [
    "Regardons maintenant de quoi a l'air notre modèle conditionné sur les données, avec les nouveaux hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e4217-5aca-46e2-9453-5a663636be61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp = build_gp(soln.params, X, yerr)\n",
    "_, cond = gp.condition(y, X_test)\n",
    "\n",
    "mu = cond.loc\n",
    "std = np.sqrt(cond.variance)\n",
    "samp = cond_gp.sample(jax.random.PRNGKey(33), shape=(5,))\n",
    "\n",
    "plt.plot(X_test, mu, label=\"Modèle\")\n",
    "plt.plot(X_test, true_y, \"k\", lw=1.5, alpha=0.3, label=\"Signal simulé\")\n",
    "plt.fill_between(X_test, mu + std, mu - std, color=\"C0\", alpha=0.3)\n",
    "plt.plot(X_test, samp.T)\n",
    "plt.errorbar(X, y, yerr, fmt=\"k.\", capsize=2)\n",
    "plt.xlim(X_test.min(), X_test.max())\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f070225-b441-4b2e-b21e-7ded5bf65417",
   "metadata": {},
   "source": [
    "Pas si mal! Par contre, difficile de voir ce qui vient du GP et ce qui vient de notre modèle gaussiens... Pour ce faire, on peut séparer les contributions de chaque terme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ede529-09a8-488e-89c0-3e6bb9ab3c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Créer un GP avec les paramètres optimisés et afficher la moyenne\n",
    "mu = None\n",
    "std = None\n",
    "mean_opt = None\n",
    "\n",
    "plt.errorbar(X, y, yerr=yerr, fmt=\"k.\", label=\"Données\", capsize=2)\n",
    "plt.plot(X_test, mu, label=\"Modèle GP\")\n",
    "plt.fill_between(X_test, mu + std, mu - std, color=\"C0\", alpha=0.3)\n",
    "# TODO: Afficher la fonction m\n",
    "plt.plot(X_test, mean_opt, label=\"Model moyen (gaussienne)\")\n",
    "\n",
    "plt.xlim(X_test.min(), X_test.max())\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1cee0-c86a-466e-9af3-77ce6d04065f",
   "metadata": {},
   "source": [
    "## HMC avec Numpyro\n",
    "\n",
    "Comme Jax nous donne automatiquement les gradients, il existe plusieurs librairies pour tirer avantage du HMC avec Jax.\n",
    "L'une des plus communes est NumPyro, qui implémente plusieurs distriubtion et simplifie beaucoup la définition d'un modèle probabilistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721312d-cb3b-4536-b387-73e0ab59ff46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "# Utiliser 2 CPUs pour le MCMC (1 par chaîne)\n",
    "numpyro.set_host_device_count(2)\n",
    "\n",
    "def numpyro_model(X, yerr, y=None):\n",
    "    # Definition de nos paramètres (priors).\n",
    "    # Comme notre modèle utilise un dictionnaire, on place les distribution dans un dictionnaire\n",
    "    params = dict(\n",
    "        # TODO: Ajouter les priors\n",
    "    )\n",
    "    \n",
    "    # Définition du modèle GP en incluant les priors\n",
    "    gp = build_gp(params, X, yerr)\n",
    "    \n",
    "    # Ceci fait office de posterior. Tinygp implémente directement une distribution numpyro\n",
    "    # En utilisant \"obs=y\", on échantillonne selon les observations.\n",
    "    # TODO: Définir la probabilité posterior\n",
    "    \n",
    "    # Il est intéressant de regarder la prédiction du GP à travers le sampling\n",
    "    if y is not None:\n",
    "        numpyro.deterministic(\"pred\", gp.condition(y, X_test).gp.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a9bbd-62e4-4186-9ad6-9b9421de6da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Numpyro utilise un \"kernel\" pour le MCMC, i.e. pour définir les propositions, un peu comme les \"moves\" emcee.\n",
    "# Ici on utilise le No U-Turn Sampling, la version la plus commune du HMC.\n",
    "# TODO: Créer un kernel \"NUTS\" pour explorer numpyro_model\n",
    "# On lance le MCMC avec 1000 warm-up (burn in) et 1000 échantillons. On utilise 2 chaines (2 walkers dans emcee). Par contre ici les chaines sont indépendantes.\n",
    "# TODO: Créer le MCMC avec 1000 warmup, 1000 steps et 2 chaines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cb392-f8bd-44b5-be4d-9252a0f42d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Avec Jax, il faut générer explicitement les nombres aléatoires\n",
    "# Exécution du MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d94323-0f91-4fe9-8ded-0b178aed44ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# On va ensuite chercher les échantillons\n",
    "samples = mcmc.get_samples()\n",
    "pred = samples[\"pred\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe20fea-7c1c-4a24-a247-aebc6b3a2b23",
   "metadata": {},
   "source": [
    "Le MCMC est terminé. On a tout ce dont on a besoin pour estimer l'incertitude sur le GP.\n",
    "\n",
    "### Visualisation\n",
    "La librairie arviz permet de visualiser facilement un résumé des résultats et les statistiques associées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0b881-3977-4a9d-a146-789f1060624f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "data = az.from_numpyro(mcmc)\n",
    "az.summary(\n",
    "    data, var_names=[v for v in data.posterior.data_vars if v != \"pred\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c658673-5b04-48fe-a9bc-8d547cd592d6",
   "metadata": {},
   "source": [
    "Le \"trace plot\" montre les chaînes et la distribution pour chaque paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd4e1c-ed3f-43e4-b2ad-9258291289db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    data, var_names=[v for v in data.posterior.data_vars if v != \"pred\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6034b9a-ca51-4e4d-86e1-6817863c61b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var_names=[v for v in data.posterior.data_vars if v != \"pred\"]\n",
    "mean_params = {\n",
    "    \"a\": 0.3,\n",
    "    \"b\": 0.1,\n",
    "    \"loc\": 5.0,\n",
    "    \"log_width\": np.log(0.5),\n",
    "}\n",
    "true_params = {name: mean_params.get(name) for name in var_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a1cfb-e359-49f5-a493-2e9cacb852f4",
   "metadata": {},
   "source": [
    "Corner est compatible avec numpyro et arviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170554da-0636-4608-9d31-38c7165c6b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "corner.corner(\n",
    "    data, var_names=[v for v in data.posterior.data_vars if v != \"pred\"], truths=true_params,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42844798-bb4e-4d90-b7f8-ca85dc42384e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "az.plot_autocorr(\n",
    "    data, var_names=[v for v in data.posterior.data_vars if v != \"pred\"]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efd6e0-9baa-4c5b-a7b1-1a77bb29922c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = np.percentile(pred, [5, 50, 95], axis=0)\n",
    "plt.fill_between(X_test, q[0], q[2], color=\"C0\", alpha=0.5, label=\"2 $\\sigma$\")\n",
    "plt.plot(X_test, q[1], color=\"C0\", lw=2, label=\"Prédiction médiane\")\n",
    "plt.plot(X_test, true_y, \"k\", lw=1.5, alpha=0.3, label=\"Signal simulé\")\n",
    "\n",
    "plt.errorbar(X, y, yerr=yerr, fmt=\".k\", capsize=2)\n",
    "plt.xlabel(\"x [day]\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(0, 10)\n",
    "plt.legend()\n",
    "plt.title(\"Posterior MCMC\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinygp-learn",
   "language": "python",
   "name": "tinygp-learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
